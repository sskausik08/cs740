\section{System Architecture}
\Cref{fig:architecture} displays the system architecture of the scheduler used
for predictive queue management in software-defined networks. The four 
components of the system are as follows: 
\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{architecture.png}
	\caption{System architecture}
	\label{fig:architecture}
\end{figure}

\begin{itemize}
	\item \textbf{SDN Controller}: The centralized point of control of the network. The 
	advantages of using SDNs for predictive queue management are two-fold:
	(1) OpenFlow has support for centralized monitoring of switches for different
	statistics, thus eliminating the need for specialized boxes for measurement and
	developing clever monitoring algorithms to minimize load on switches due to 
	querying statistics (2) The controller can add/modify forwarding rules at switches,
	thus providing a centralized point to decide routing of flows, which is received as
	output from the dynamic scheduler to prevent queue buildups. 
	
	\item \textbf{Network Monitor}: Based on the monitoring algorithm used at the
	controller, statistics from the entire network topology is collected and passed to the
	network monitor, which store the necessary statistics needed to predict queue buildups in switches.
	
	\item \textbf{Flow Classifier}: Using the characteristics gathered by the network monitor,
	the flow classifier will classify the flow as mice/elephant based on some threshold (can be user-defined)
	
	\item \textbf{Dynamic Scheduler}: The scheduler predicts queue buildups using the characteristics
	from the network monitor, and dynamically schedules paths for flows to prevent queue buildups affecting
	latency-sensitive mice flows using an earliest-deadline-first greedy scheduling algorithm to find new paths,
	which are sent to the controller to be deployed to the network.
\end{itemize}
We describe the components in detail in the following sections.
\section{Flow Characteristics}
\begin{figure}[H]
	\centering
	\includegraphics[width=\columnwidth]{flowchar.png}
	\caption{Flow Characteristic Model}
	\label{fig:flowchar}
\end{figure}
To predict how queue builds up, we need to develop a model of a flow 
such that we can predict the data it sends in any duration of time. We 
characterize each flow as following an repeating ON/OFF model \cite{datacentertraffic}
as shown in \Cref{fig:flowchar}. The three main parameters are:
\begin{enumerate}
	\item \textbf{Active time($T_{ON}$)}: The duration of time 
	for which the flow is actively transmitting data
	\item \textbf{Inactive time($T_{OFF}$)}: The duration of time 
	for which the flow is inactive between two transmission cycles
	\item \textbf{Transmission Bytes($\Omega$)}: The total number
	of bytes sent in a single transmission period (the rate is assumed
	to be constant in the period)
\end{enumerate}
By maintaining a model for a flow, we can use this to easily estimate
the number of bytes sent by this flow in a time interval. The queue
buildup model can be built by using this information from all flows
going through the queue. Development of sophisticated flow characteristic
models based on specific distributions is a direction of future work. 

\subsection{Measurement}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\columnwidth]{meastrace.png}
	\caption{Example measurement trace for a flow. An interval where the total bytes change 
		is marked as an ON period, and conversely, an OFF period means the reading is constant.}
	\label{fig:meastrace}
\end{figure}
We use OpenFlow's support for obtaining flow statistics from 
switches to estimate the flow characteristic parameters. An example
measurement trace for a flow is shown in \Cref{fig:meastrace}. In
every $t_{meas}$, the edge switch where the flow enters the network,
 is queried for the statistics of the 
flow, which returns the cumulative bytes received at the switch. We maintain
a expotential moving average(EWMA) for each parameter, and update the average
from each reading. 

The main advantages of this approach is that by measuring flow
characteristics, we do not need cooperation from the tenants to provide information about
flows (which may be difficult in public clouds and can be misused by malicious tenants). The 
major shortcoming of this approach is that the least count of $T_{ON}$ and $T_{OFF}$ is
the query time interval $t_{meas}$ (we cannot say anything about a time inside a query interval).
Thus, it is a crucial trade-off between accuracy of the flow characteristics and the load on switches
due to frequent querying. 

\subsection{Path Characteristic}
While we measure the characteristic of the flow at the source edge switch, 
the flow characteristics would change as the flow traverses through the network.
Let us consider a single switch. If a flow sent data at a rate greater than the output
bandwidth, the rate of the output flow would be truncated by the output capacity. 
If there is no losses at switch, then the output $\Omega$ would remain
constant. Using this, we can find the output $T_{ON}$ and $T_{OFF}$, which
would be different from the input characteristic. However, instead of measuring 
this at each switch (which would lead to increased load of querying), we predict
the path characteristics of the flow as it flows through the network. 

For estimating the path characteristics of the flow, we assume two things: (1) No losses
at the switch- our predictions would be more conservative than if there were losses
(2) No fair queuing at switches- the output bandwidth is alloted in proportion to the input
rates of the flows, so if a flow sends a higher rate, then it would get a larger share of the output 
bandwidth. 

\begin{figure}[H]
	\centering
	\includegraphics[width=\columnwidth]{switchmodel.png}
	\caption{Illustration of a switch queue model}
	\label{fig:switchmodel}
\end{figure}

Consider two switch $sw_i$ and $sw_j$(\Cref{fig:switchmodel}) and let the out link
capacity connecting $sw_i \rightarrow sw_j$ to be $C_{ij}^{out}$. Let $Flows$
denote the set of flows traversing $sw_i \rightarrow sw_j$. Consider a flow $f$ at $sw_i$ 
with characteristics $\Omega, T_{ON}, T_{OFF}$. The output characteristics $\Omega',T_{ON}', T_{OFF}'$  are described as follows: 
\begin{equation}
	\Omega' = \Omega
\end{equation} 
The output rate is proportional to the proportion of input data sent by $f$. The data sent by
$f$ in a active period is $\Omega$. Let $\Omega(t)$ denote the bytes sent by the flow in time $t$.
We decide the proportion by the amount of data sent by other flows sent in $T_{ON}$ and
divide the output capacity using this proportion. Thus, the estimated output rate $R'$ is defined as follows: 
\begin{equation}
	R' = min(\ \frac{\Omega}{T_{ON}}, \ \  C_{ij}^{out} \times \frac{\Omega}{\sum_{f \in Flows} \Omega_f(T_{ON}) })
\end{equation}
The $min$ operation is to ensure that the output rate does not increase the input rate,
the flow cannot accelerate at any point in the path. With the estimated output rate, we can find the other parameters of the characteristic. As the rate will decrease, the active time will increase to sent the $\Omega$ bytes.
\begin{equation}
	T_{ON}' = \frac{\Omega}{R'}
\end{equation}
The inactive times will reduce because of the increase in active times. 
\begin{equation}
	T_{OFF}' = T_{OFF} - ( T_{ON}'  - T_{ON} )
\end{equation}
Thus, by measuring flow statistics and using these equations, we have a comprehensive picture of characteristics of flows on all switches of the topology. In the next section, we explain how we use these to predict queue buildups.

\subsection{Flow Classifier}
In our current iteration, we have defined two set of classes (elephant and mice), and defined 
a configurable threshold on $\Omega$ and $T_{ON}$ to classify a flow. However, there can
exist multiple classes of flows with different priorities, and the scheduling can be modified to
take into account the priorities.

\section{Queue Buildup Model}
As discussed earlier, latency-sensitive mice flows are affected when a switch queue increases beyond
some threshold. Instead of reactively responding to these events, which may be difficult, we 
predictively try to ensure that mice flows are not affected by these queue buildups. To perform this,
we need a model of queue builds up. Using the same example in \Cref{fig:switchmodel}, we define $Q_{ij}(t)$ as the function of queue size of $sw_i \rightarrow sw_j$ over time t as:
\begin{equation}
	Q_{ij}(t) = Q_{ij}(t=0) - C_{ij}^{out} \times t + \sum_{f \in Flows} \Omega_f(t)
\end{equation}
The intuition behind the above equation is that increase is queue size from initial
queue size ($Q_{ij}(t=0)$) is the difference of bytes received in the queue ($\sum_{f \in Flows} \Omega_f(t)$) and the bytes send out the queue ($C_{ij}^{out} \times t $). 

OpenFlow has no support for querying current queue size in the switch, thus
there is no direct way to measure the queue size at any point of time. So, to 
predict initial size, we query flow statistics for $Flows$ at $sw_i$ and $sw_j$,
and the difference between the statistics is a good estimate of current queue size.

Finally, we define the critical time $t_c$ for a switch when $Q_{ij}(t_c) > threshold$ such 
that there are mice flows affected by the queue buildup.  This 
threshold can be defined based on latency limits or can be set to total queue size to
denote congestion events. The critical threshold can also depend on the number of 
mice flows being affected (can ignore small number of mice flows being affected in large
networks).

\section{Scheduling Algorithm}
By building a predictive model of queues, we can manage the flows effectively
and ahead of time, instead of reacting to failures or higher latencies. Another advantage 
of this approach is that we can run the scheduler at larger epochs, thus reducing the
overload of scheduling. 

Let us formally define the objectives of scheduling. The input to the scheduler
is the set of flows and their paths along the network, with the estimated  
characteristics for each flow. Also, using the queue buildup model of each switch
queue and certain user-defined objectives of critical threshold, we can obtain the
critical times for each of switches, if the flows maintain their current paths. 

The scheduler is invoked in every $t_{sched}$ epoch, and uses a 
earliest-deadline-first algorithm, which basically means we only consider
switches with critical times $< t_{sched}$ within the current time. The rationale 
behind this is that these switches will be critical before the scheduler is invoked 
again, so require attention in this iteration. 

Once we have identified the switches in critical mode this epoch,
the next step of the scheduling algorithm is to reroute certain flows 
traversing these switches along paths 
such that the switches will not be critical in this epoch. For this, we use 
a greedy depth-first search in the network to find suitable reroute paths for
flows. For choosing the flows to reroute, we choose elephant flows over mice flows,
as rerouting a single elephant can dramatically change the critical time at the switch.
Also to prevent route-flapping, we do not choose flows which was rerouted in the previous
two iterations of the scheduler. 

Once we choose a flow, we now decide a new route for this flow. We explore the 
network from the source switch in a depth-first manner such that we recompute the
critical time for the switch if this flow was also added along the switch. If a switch 
is or becomes critical within this epoch, we do not explore this switch further (\emph{unfeasible} path). This search is greedy because we will terminate once we find a path from source
 to destination (or leave the flow's path as it is). This way, we always do better than 
 a naive scheduling algorithm which does not take into consideration queue buildups.  











